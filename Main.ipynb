{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Чтение исходных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPATH = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = DATAPATH + 'train.json'\n",
    "test_file = DATAPATH + 'test.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id sentiment\n",
       "0   0   neutral\n",
       "1   1  positive\n",
       "2   2  negative\n",
       "3   3  positive\n",
       "4   4   neutral"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = pd.read_csv(DATAPATH + 'sample.csv')\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(json_filename):\n",
    "    with open(json_filename) as json_file:\n",
    "        temp = json.load(json_file)\n",
    "        return pd.DataFrame.from_records(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Досудебное расследование по факту покупки ЕНПФ...</td>\n",
       "      <td>1945</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Медики рассказали о состоянии пострадавшего му...</td>\n",
       "      <td>1957</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Прошел почти год, как железнодорожным оператор...</td>\n",
       "      <td>1969</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>По итогам 12 месяцев 2016 года на территории р...</td>\n",
       "      <td>1973</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Астана. 21 ноября. Kazakhstan Today - Агентств...</td>\n",
       "      <td>1975</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    id sentiment\n",
       "0  Досудебное расследование по факту покупки ЕНПФ...  1945  negative\n",
       "1  Медики рассказали о состоянии пострадавшего му...  1957  negative\n",
       "2  Прошел почти год, как железнодорожным оператор...  1969  negative\n",
       "3  По итогам 12 месяцев 2016 года на территории р...  1973  negative\n",
       "4  Астана. 21 ноября. Kazakhstan Today - Агентств...  1975  negative"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = get_df(train_file)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Как сообщает пресс-служба акимата Алматы, для ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Казахстанские авиакомпании перевозят 250 тысяч...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>На состоявшемся под председательством Касым-Жо...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>В ОАЭ состоялись переговоры между казахстанско...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>12 вагонов грузового поезда сошли с путей в Во...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  id\n",
       "0  Как сообщает пресс-служба акимата Алматы, для ...   0\n",
       "1  Казахстанские авиакомпании перевозят 250 тысяч...   1\n",
       "2  На состоявшемся под председательством Касым-Жо...   2\n",
       "3  В ОАЭ состоялись переговоры между казахстанско...   3\n",
       "4  12 вагонов грузового поезда сошли с путей в Во...   4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = get_df(test_file)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предобработка с помощью UDPipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Установка нужных пакетов и загрузка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_udpipe_model():\n",
    "    #!pip install ufal.udpipe\n",
    "    #!pip install wget\n",
    "    udpipe_url = 'https://rusvectores.org/static/models/udpipe_syntagrus.model'\n",
    "    udpipe_filename = udpipe_url.split('/')[-1]\n",
    "\n",
    "    if not os.path.isfile(udpipe_filename):\n",
    "        print('UDPipe model not found. Downloading...', file=sys.stderr)\n",
    "        wget.download(udpipe_url)\n",
    "    \n",
    "    return udpipe_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестовые тексты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_url = 'https://rusvectores.org/static/henry_sobolya.txt'\n",
    "text_filename = text_url.split('/')[-1]\n",
    "\n",
    "if not os.path.isfile(text_filename):\n",
    "    print('{} not found. Downloading...'.format(text_filename), file=sys.stderr)\n",
    "    wget.download(text_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text_0 = 'Пока Ольга, Максим и Никита разговаривали по телефону 81234567890, моя мама Людмила тщательно мыла \\\n",
    "грязную раму картины \"Последний день Владимира\" \\\n",
    "из Третьяковки и Эрмитажа новым мылом Nivea от Яндекса и Газпрома до блеска и сверкания для передачи в \\\n",
    "Москву, в Валенсию, Puma, Пермь, NFL и Екатеринбург'\n",
    "test_text_1 = train['text'][0]\n",
    "test_text_2 = train['text'][1]\n",
    "test_text_3 = open(text_filename, 'r', encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Получение Conllu формата"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ufal.udpipe import Model, Pipeline\n",
    "import unify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def load_udpipe_pipeline(udpipe_filename):\n",
    "    model = Model.load(udpipe_filename)    \n",
    "    process_pipeline = Pipeline(model, 'tokenize', Pipeline.DEFAULT, Pipeline.DEFAULT, 'conllu')\n",
    "    return model, process_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def clean_token(token, misc):\n",
    "    \"\"\"\n",
    "    :param token:  токен (строка)\n",
    "    :param misc:  содержимое поля \"MISC\" в CONLLU (строка)\n",
    "    :return: очищенный токен (строка)\n",
    "    \"\"\"\n",
    "    out_token = token.strip().replace(' ', '')\n",
    "    if token == 'Файл' and 'SpaceAfter=No' in misc:\n",
    "        return None\n",
    "    return out_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def clean_lemma(lemma, pos):\n",
    "    \"\"\"\n",
    "    :param lemma: лемма (строка)\n",
    "    :param pos: часть речи (строка)\n",
    "    :return: очищенная лемма (строка)\n",
    "    \"\"\"\n",
    "    out_lemma = lemma.strip().replace(' ', '').replace('_', '').lower()\n",
    "    if '|' in out_lemma or out_lemma.endswith('.jpg') or out_lemma.endswith('.png'):\n",
    "        return None\n",
    "    if pos != 'PUNCT':\n",
    "        if out_lemma.startswith('«') or out_lemma.startswith('»'):\n",
    "            out_lemma = ''.join(out_lemma[1:])\n",
    "        if out_lemma.endswith('«') or out_lemma.endswith('»'):\n",
    "            out_lemma = ''.join(out_lemma[:-1])\n",
    "        if out_lemma.endswith('!') or out_lemma.endswith('?') or out_lemma.endswith(',') \\\n",
    "                or out_lemma.endswith('.'):\n",
    "            out_lemma = ''.join(out_lemma[:-1])\n",
    "    return out_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def num_replace(word):\n",
    "    newtoken = 'x' * len(word)\n",
    "    nw = newtoken + '_NUM'\n",
    "    return nw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def process(pipeline, text='Строка', keep_pos=True, keep_punct=False):\n",
    "    entities = {'PROPN'}\n",
    "    named = False\n",
    "    memory = []\n",
    "    mem_case = None\n",
    "    mem_number = None\n",
    "    tagged_propn = []\n",
    "    \n",
    "    # обрабатываем текст, получаем результат в conllu\n",
    "    processed = process_pipeline.process(text)\n",
    "\n",
    "    # пропускаем строки со служебной информацией\n",
    "    content = [l for l in processed.split('\\n') if not l.startswith('#')]\n",
    "\n",
    "    # извлекаем из обработанного текста леммы, тэги и морфологические характеристики\n",
    "    tagged = [w.split('\\t') for w in content if w]\n",
    "\n",
    "    result = tagged\n",
    "\n",
    "    # print(result)\n",
    "    # print('='*100)\n",
    "\n",
    "    # print(len(tagged))\n",
    "    # print('='*100)\n",
    "\n",
    "    for t in tagged:\n",
    "    #     print(t, named)\n",
    "        if len(t) != 10:\n",
    "            continue\n",
    "\n",
    "        (word_id, token, lemma, pos, xpos, feats, head, deprel, deps, misc) = t\n",
    "\n",
    "        # убрали внутри токена оконечные и внутренние пробелы, перевели в нижний регистр\n",
    "        token = clean_token(token, misc)\n",
    "        # убрали внутри леммы оконечные и внутренние пробелы, перевели в нижний регистр, \\\n",
    "        # убрали оконечные знаки преминания\n",
    "        lemma = clean_lemma(lemma, pos)\n",
    "\n",
    "        if not lemma or not token:\n",
    "            continue\n",
    "\n",
    "        if pos in entities:\n",
    "            # если у имени собственного нет морфологических характеристик или она только одна\n",
    "            if '|' not in feats:\n",
    "                tagged_propn.append('{}_{}'.format(lemma, pos))\n",
    "                continue # модификация - в оригинале этого нет\n",
    "\n",
    "            # если нет падежа или числа\n",
    "            morph = {el.split('=')[0]: el.split('=')[1] for el in feats.split('|')}\n",
    "            if 'Case' not in morph or 'Number' not in morph:\n",
    "                tagged_propn.append('{}_{}'.format(lemma, pos))\n",
    "                continue\n",
    "\n",
    "            if not named:\n",
    "                named = True\n",
    "                mem_case = morph['Case']\n",
    "                mem_number = morph['Number']\n",
    "\n",
    "    #         print(token, morph, morph['Case'], mem_case, morph['Number'], mem_number, memory)\n",
    "\n",
    "            # группируем все имена собственные одного падежа и числа в одну сущность\n",
    "            if morph['Case'] == mem_case and morph['Number'] == mem_number:\n",
    "                memory.append(lemma)\n",
    "    #             print(lemma, memory)\n",
    "                if 'SpacesAfter=\\\\n' in misc or 'SpacesAfter=\\s\\\\n' in misc:\n",
    "                    named = False\n",
    "                    past_lemma = '::'.join(memory)\n",
    "                    memory = []\n",
    "                    tagged_propn.append(past_lemma + '_PROPN ')\n",
    "            else:\n",
    "                named = False\n",
    "                past_lemma = '::'.join(memory)\n",
    "                memory = []\n",
    "                tagged_propn.append(past_lemma + '_PROPN ')\n",
    "                tagged_propn.append('{}_{}'.format(lemma, pos))\n",
    "\n",
    "        else:\n",
    "            if not named:\n",
    "                # если число, то заменяем цифры на x и добавляем окончание _NUM в лемму\n",
    "                if pos == 'NUM' and token.isdigit():\n",
    "                    lemma = num_replace(token)\n",
    "                tagged_propn.append('{}_{}'.format(lemma, pos))\n",
    "            else:\n",
    "                named = False\n",
    "                past_lemma = '::'.join(memory)\n",
    "                memory = []\n",
    "                tagged_propn.append(past_lemma + '_PROPN ')\n",
    "                tagged_propn.append('{}_{}'.format(lemma, pos))    \n",
    "\n",
    "    if not keep_punct:\n",
    "        tagged_propn = [word for word in tagged_propn if word.split('_')[1] != 'PUNCT']\n",
    "\n",
    "    if not keep_pos:\n",
    "        tagged_propn = [word.split('_')[0] for word in tagged_propn]\n",
    "\n",
    "    return tagged_propn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пока Ольга, Максим и Никита разговаривали по телефону 81234567890, моя мама Людмила тщательно мыла грязную раму картины \"Последний день Владимира\" из Третьяковки и Эрмитажа новым мылом Nivea от Яндекса и Газпрома до блеска и сверкания для передачи в Москву, в Валенсию, Puma, Пермь, NFL и Екатеринбург\n",
      "====================================================================================================\n",
      "['пока', 'ольга', 'максим', 'и', 'никита', 'разговаривать', 'по', 'телефон', 'xxxxxxxxxxx', 'мой', 'мама', 'людмила', 'тщательно', 'мыть', 'грязный', 'рама', 'картина', 'последний', 'день', 'владимир', 'из', 'третьяковка', 'и', 'эрмитаж', 'новый', 'мыло', 'nivea', 'от', 'яндекс', 'и', 'газпром', 'до', 'блеск', 'и', 'сверкание', 'для', 'передача', 'в', 'москва', 'в', 'валенсия', 'puma', 'пермь', 'nfl', 'и', 'екатеринбург']\n"
     ]
    }
   ],
   "source": [
    "udpipe_filename = download_udpipe_model()\n",
    "model, process_pipeline = load_udpipe_pipeline(udpipe_filename)\n",
    "\n",
    "line = unify.unify_sym(test_text_0)\n",
    "print(line)\n",
    "print('='*100)\n",
    "\n",
    "output = process(process_pipeline, text=line, keep_pos=False, keep_punct=False)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.text\n",
    "y = train.sentiment\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB())\n",
    "              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 889 ms, sys: 4.66 ms, total: 894 ms\n",
      "Wall time: 761 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "y_pred = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.58\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: {:.2f}'.format(accuracy_score(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.03      0.07       435\n",
      "    positive       0.54      0.97      0.70      1226\n",
      "     neutral       0.83      0.27      0.41       818\n",
      "\n",
      "    accuracy                           0.58      2479\n",
      "   macro avg       0.72      0.43      0.39      2479\n",
      "weighted avg       0.68      0.58      0.49      2479\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=y.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gensim, logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-01-20 16:50:26,155 : INFO : loading projection weights from <zipfile.ZipExtFile name='model.bin' mode='r' compress_type=deflate>\n",
      "2020-01-20 16:50:34,232 : INFO : loaded (189193, 300) matrix from <zipfile.ZipExtFile [closed]>\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "model_url = 'http://vectors.nlpl.eu/repository/11/180.zip'\n",
    "m = wget.download(model_url)\n",
    "model_file = model_url.split('/')[-1]\n",
    "with zipfile.ZipFile(model_file, 'r') as archive:\n",
    "    stream = archive.open('model.bin')\n",
    "    model = gensim.models.KeyedVectors.load_word2vec_format(stream, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words = ['день_NOUN', 'ночь_NOUN', 'человек_NOUN', 'семантика_NOUN', 'студент_NOUN', 'студент_ADJ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for word in words:\n",
    "#     # есть ли слово в модели? Может быть, и нет\n",
    "#     if word in model:\n",
    "#         print(word)\n",
    "#         # выдаем 10 ближайших соседей слова:\n",
    "#         for i in model.most_similar(positive=[word], topn=10):\n",
    "#             # слово + коэффициент косинусной близости\n",
    "#             print(i[0], i[1])\n",
    "#         print('\\n')\n",
    "#     else:\n",
    "#         # Увы!\n",
    "#         print(word + ' is not present in the model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.similarity('человек_NOUN', 'обезьяна_NOUN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.doesnt_match('яблоко_NOUN груша_NOUN виноград_NOUN банан_NOUN лимон_NOUN картофель_NOUN'.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.most_similar(positive=['пицца_NOUN', 'россия_NOUN'], negative=['италия_NOUN'])[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "285.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "905px",
    "left": "-35px",
    "right": "20px",
    "top": "260px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
